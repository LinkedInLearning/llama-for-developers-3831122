{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Launching the server\n",
        "The recommended way to Launch TGI is through docker. Unfortunetly we cannot natively run docker in collab, but on a seperate VM you can start it up.\n",
        "\n",
        "### Reasons for using docker\n",
        "- Ease of use\n",
        "- Cuda dependencies\n",
        "- Rust dependencies"
      ],
      "metadata": {
        "id": "V1gvT6AMcNxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_I_BefIbmuW"
      },
      "outputs": [],
      "source": [
        "# set up env variables\n",
        "!model=meta-llama/Llama-2-7b-chat-hf\n",
        "!volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n",
        "!token=\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the server using GPUs\n",
        "!docker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model\n"
      ],
      "metadata": {
        "id": "YeK94Z9jbxRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running TGI in collab\n",
        "Running a LLM inferencing server isnt the best idea on a jupyter notebook, but if you really want to, i'd recommend reading this [guide](https://towardsdatascience.com/llms-for-everyone-running-the-huggingface-text-generation-inference-in-google-colab-5adb3218a137)"
      ],
      "metadata": {
        "id": "suqKCFB7dM5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sending a request to the server"
      ],
      "metadata": {
        "id": "PRhXRED1dkMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have the server running you can do a standard REST request to the local port."
      ],
      "metadata": {
        "id": "aLv7S9DQcwkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "data = {\n",
        "    'inputs': 'What is Deep Learning?',\n",
        "    'parameters': {\n",
        "        'max_new_tokens': 20,\n",
        "    },\n",
        "}\n",
        "\n",
        "response = requests.post('http://127.0.0.1:8080/generate', headers=headers, json=data)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "6UGakAyubyEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}