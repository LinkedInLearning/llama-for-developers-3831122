{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning a few LLaMa 7B layers for sentiment classification\n",
        "1. Install dependencies\n",
        "2. Load model\n",
        "3. Freeze layers\n",
        "4. Create instruction tuned dataset\n",
        "5. Test prompt\n",
        "6. Run training with smaller optimizer\n",
        "7. Test prompt"
      ],
      "metadata": {
        "id": "UrhR47l7Bkhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How much memory is required for training?\n",
        "1. Model parameters - 4 bytes per model parameter\n",
        "2. Optimizer - 8 bytes per model parameter\n",
        "3. Gradient - 4 bytes per model parameter\n",
        "\n",
        "7B Parameters * (4+8+4) = 112 GB of RAM!\n",
        "\n",
        "### Can reduce by:\n",
        "1. Freezing layers to reduce trainable parameters\n",
        "2. Quantize model/reduce precision\n",
        "3. Use smaller optimizer\n",
        "\n",
        "Goal is to get to less than 40GB, A100 has 40GB, T4 has 16GB\n",
        "\n",
        "Read more at:\n",
        "https://huggingface.co/docs/transformers/model_memory_anatomy#anatomy-of-models-memory"
      ],
      "metadata": {
        "id": "IdziilNYLk6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets bitsandbytes trl"
      ],
      "metadata": {
        "id": "JeS0wmLAAaER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "tPw39U8BHIJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DivrVBQ-I09p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "train_ds, test_ds = load_dataset('imdb', split=['train[:2%]+train[-2%:]', 'test[:2%]+test[-2%:]'])\n",
        "\n",
        "sentiment_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "Classify the following sentiment into a number.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"text\"]\n",
        "    outputs      = examples[\"label\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        text = sentiment_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "train_ds = train_ds.shuffle(seed=42).map(formatting_prompts_func, batched = True,)\n",
        "test_ds = test_ds.shuffle(seed=42).map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "bsz32_0PAoNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ds)\n",
        "print(test_ds)\n",
        "print(train_ds[\"text\"][5])\n",
        "print(train_ds[\"label\"][:10])"
      ],
      "metadata": {
        "id": "WB-JtxJDRd96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add model freezing code"
      ],
      "metadata": {
        "id": "UguEOzHhSsDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFkeqLytAKLg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer,TrainingArguments\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    optim = \"adamw_8bit\",\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 10,\n",
        "    )\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=256,\n",
        "    args=args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "7MSJfWZtAVQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "Classify the following sentiment into a number.\n",
        "\n",
        "### Input:\n",
        "It was a great movie!\n",
        "\n",
        "### Response:\"\"\"\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "with torch.cuda.amp.autocast():\n",
        "  output = model.generate(**model_inputs,max_new_tokens=50)"
      ],
      "metadata": {
        "id": "WCLBrYixhENH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4YX8H8GnuBZ",
        "outputId": "943cf9fb-e6e3-4cfe-e4a3-76da21dd637d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "### Instruction:\n",
            "Classify the following sentiment into a number.\n",
            "\n",
            "### Input:\n",
            "It was a great movie!\n",
            "\n",
            "### Response:\n",
            "1\n",
            "\n",
            "### Instruction:\n",
            "Classify the following sentiment into a number.\n",
            "\n",
            "### Input:\n",
            "It was a bad movie!\n",
            "\n",
            "### Response:\n",
            "0\n",
            "\n",
            "### Instruction:\n",
            "Classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H--AdLNFUwCP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}